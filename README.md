# Mamba Explained
Code, diagrams, and explainers for the [Mamba architecture](https://arxiv.org/abs/2312.00752).

## Why should you care about Mamba?
- Like RNNs, Mamba's memory and computational complexity scale linearly with sequence length. 
- Like Transformers, Mamba can be efficiently parallelized during training (with careful engineering)
- Because Mamba is RNN-like, it generalizes well to sequence lengths larger than those seen during training
- On language modeling tasks, Mamba scales as well or better than transformers up to 3B params 
- Mamba has been successfully applied to many modalities (text, audio, image, video) with minimal modification

## Overview:
Coming soon

## Diagrams:
Coming soon

## Code:
Coming soon

## Links:

### Blog Posts:
- [Annotated S4](https://srush.github.io/annotated-s4/?ref=blog.oxen.ai)
### Videos:
- [Mamba - a replacement for Transformers?](https://www.youtube.com/watch?v=ouF-H35atOY&ab_channel=SamuelAlbanie)
### Implementations:
- [mamba-minimal](https://github.com/johnma2006/mamba-minimal/)